{
  "og": {
"og_keywords": "AI, Deep Learning, Neural Networks",
"og_bibliography":"A. Amanlou, A. A. Suratgar, J. Tavoosi, A. Mohammadzadeh, and A. Mosavi. Single-image reflection removal using deep learning: A systematic review. IEEE Access, 10:29937–29953, 2022. doi: 10. 1109/ACCESS.2022.3156273.H. Avron and S. Toledo. Randomized algorithms for estimating the trace of an implicit symmetricpositive semi-definite matrix. Journal of the ACM (JACM), 58(2):1–34, 2011.L. Bungert, T. Roith, D. Tenbrinck, and M. Burger. A bregman learning framework for sparse neuralnetworks. arXiv preprint arXiv:2105.04319, 2021.K. Cho, B. van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder–decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724–1734. Association for Computational Linguistics, Oct. 2014. T. DeVries and G. W. Taylor. Improved regularization of convolutional neural networks with Cutout. arXiv preprint arXiv:1708.04552, 2017. L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio. Sharp minima can generalize for deep nets. In International Conference on Machine Learning, pages 1019–1028. PMLR, 2017. Z. Ebrahimi-Khusfi, R. Taghizadeh-Mehrjardi, F. Roustaei, M. Ebrahimi-Khusfi, A. H. Mosavi, B. Heung, M. Soleimani-Sardo, and T. Scholten. Determining the contribution of environmental factors in controlling dust pollution during cold and warm months of western iran using different data mining algorithms and game theory. Ecological Indicators, 132:108287, 2021. ISSN 1470-160X. doi: https://doi.org/10.1016/j.ecolind.2021.108287. URL https://www.sciencedirect.com/science/ article/pii/S1470160X21009523. G. Ghiasi, T.-Y. Lin, and Q. V. Le. Dropblock: A regularization method for convolutional networks. Advances in Neural Information Processing Systems, 31:10727–10737, 2018. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. D. P. Helmbold and P. M. Long. On the inductive bias of dropout. The Journal of Machine Learning Research, 16(1):3403–3454, 2015. S. Hochreiter and J. Schmidhuber. Flat minima. Neural computation, 9(1):1–42, 1997a. S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997b. A. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1):55–67, 1970. J. Hoffman, D. A. Roberts, and S. Yaida. Robust learning with jacobian regularization. Conference on the Mathematical Theory of Deep Learning (DeepMath), 2019. N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016. A. Krogh and J. Hertz. A simple weight decay can improve generalization. Advances in Neural Information Processing Systems, 4, 1992.",
"og_literature_review":"There are many regularization methods in previous work. Label Smoothing [Szegedy et al., 2016] estimates the marginalized effect of label-dropout and reduces over-fitting by preventing a network from assigning full probability to each training example. Confidence Penalty [Pereyra et al., 2017] prevents peaked distributions, leading to better generalization. A network appears to be overconfident when it places all probability on a single class in the training set, which is often a symptom of overfitting. DropBlock [Ghiasi et al., 2018] is a structured form of dropout, it drops contiguous regions from a feature map of a layer instead of dropping out independent random units. Data augmentation methods are also used in practice to improve model’s accuracy and robustness when training neural networks."
},
  "sus": {
"sus_keywords": "AI, Deep Learning, Neural Networks, Regularization, Dropout, Weight Decay",
"sus_bibliography":"A. Amanlou, A. A. Suratgar, J. Tavoosi, A. Mohammadzadeh, and A. Mosavi. Single-image reflection removal using deep learning: A systematic review. IEEE Access, 10:29937–29953, 2022. doi: 10. 1109/ACCESS.2022.3156273. H. Avron and S. Toledo. Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix. Journal of the ACM (JACM), 58(2):1–34, 2011. L. Bungert, T. Roith, D. Tenbrinck, and M. Burger. A bregman learning framework for sparse neural networks. arXiv preprint arXiv:2105.04319, 2021. K. Cho, B. van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder–decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724–1734. Association for Computational Linguistics, Oct. 2014. T. DeVries and G. W. Taylor. Improved regularization of convolutional neural networks with Cutout. arXiv preprint arXiv:1708.04552, 2017. L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio. Sharp minima can generalize for deep nets. In International Conference on Machine Learning, pages 1019–1028. PMLR, 2017. Z. Ebrahimi-Khusfi, R. Taghizadeh-Mehrjardi, F. Roustaei, M. Ebrahimi-Khusfi, A. H. Mosavi, B. Heung, M. Soleimani-Sardo, and T. Scholten. Determining the contribution of environmental factors in controlling dust pollution during cold and warm months of western iran using different data mining algorithms and game theory. Ecological Indicators, 132:108287, 2021. ISSN 1470-160X. doi: https://doi.org/10.1016/j.ecolind.2021.108287. URL https://www.sciencedirect.com/science/ article/pii/S1470160X21009523. G. Ghiasi, T.-Y. Lin, and Q. V. Le. Dropblock: A regularization method for convolutional networks. Advances in Neural Information Processing Systems, 31:10727–10737, 2018. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition",
"sus_literature_review": "Previous work contains numerous regularisation methods. Label Smoothing [Szegedy et al., 2016] estimates the marginalised effect of label-dropout and prevents over-fitting by preventing a network from assigning full probability to every training example. [Pereyra et al., 2017] Confidence Penalty prevents peaked distributions, resulting in improved generalisation A network appears to be arrogant. when it assigns all probability to a single class in the training set, which is frequently a sign of overfitting DropBlock [Ghiasi et al., 2018] is a structured dropout that eliminates contiguous regions. instead of dropping out independent random units from a layer's feature map In practise, data augmentation methods are also used to improve model accuracy and robustness. when neural networks are being trained"
},
"type": 0
}