{
  "og": {},
  "sus": {"data": "Regularizing Deep Neural Networks with Stochastic Estimators of Hessian Trace In this paper we develop a novel regularization method for deep neural networks by penalizing the trace of Hessian. This regularizer is motivated by a recent guarantee bound of the generalization error. Hutchinson method is a classical unbiased estimator for the trace of a matrix, but it is very time-consuming on deep learning models. Hence a dropout scheme is proposed to efficiently implements the Hutchinson method. Then we discuss a connection to linear stability of a nonlinear dynamical system and flat/sharp minima. Experiments demonstrate that our method outperforms existing regularizers and data augmentation methods, such as Jacobian, confidence penalty, and label smoothing, cutout and mixup. Deep neural networks (DNNs) are developing rapidly and are widely used in many fields such as reflection removal [Amanlou et al., 2022], dust pollution [Ebrahimi-Khusfi et al., 2021], building defects detection [Perez et al., 2019], cities and urban development [Nosratabadi et al., 2020]. As more and more models are proposed in the literature, deep neural networks have shown remarkable improvements in performance. However, among various learning problems, over-fitting on training data is a great problem that affects the test accuracy. So a certain regularization method is often needed in the training process. In linear models, Ridge Regression [Hoerl and Kennard, 1970] and Lasso [Tibshirani, 1996] are usually used to avoid over-fitting. They are also called L2 and L1 regularization. L2 regularization has the effect of shrinkage while L1 regularization can be conductive to both shrinkage and sparsity. From the Bayesian perspective, L2 and L1 regularization can be interpreted with normal prior distribution and laplace prior distribution respectively. Apart from L2 and L1 regularization, there are many other forms of regularizers in DNNs. The most widely used one is Weight-Decay [Krogh and Hertz, 1992]. Loshchilov and Hutter [2019] also showed that L2 regularization and Weight-Decay are not identical. Dropout [Srivastava et al., 2014] is another method to avoid over-fitting by reducing co-adapting between units in neural networks. Dropout has inspired a large body of work studying its effects (Wager et al. [2013]; Helmbold and Long [2015]; Wei et al. [2020]). After dropout, various regularization schemes can be applied additionally. AI, Deep Learning, Neural Networks We propose a new regularization method for deep neural networks by penalizing the trace of Hessian. This regularizer is motivated by a recent guarantee bound of the generalization error. Hutchinson method is a classical unbiased estimator for the trace of a matrix, but it is very time-consuming on deep learning models. Hence a dropout scheme is proposed to efficiently implements the Hutchinson method. Then we discuss a connection to linear stability of a nonlinear dynamical system and flat/sharp minima. Experiments demonstrate that our method outperforms existing regularizers and data augmentation methods, such as Jacobian, confidence penalty, and label smoothing, cutout and mixup."
},
  "type": 1,
  "apikey": "AIzaSyBzLQiTsvKVY_HlF7qDn0o7OO-_HD-iyDc"
}