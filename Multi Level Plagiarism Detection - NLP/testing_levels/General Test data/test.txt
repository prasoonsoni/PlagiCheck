{
  "og": {"og_title": "Regularizing Deep Neural Networks with Stochastic Estimators of Hessian Trace ",
        "og_ps_obj": "In this paper we develop a novel regularization method for deep neural networks by penalizing the trace of Hessian. This regularizer is motivated by a recent guarantee bound of the generalization error. Hutchinson method is a classical unbiased estimator for the trace of a matrix, but it is very time-consuming on deep learning models. Hence a dropout scheme is proposed to efficiently implements the Hutchinson method. Then we discuss a connection to linear stability of a nonlinear dynamical system and flat/sharp minima. Experiments demonstrate that our method outperforms existing regularizers and data augmentation methods, such as Jacobian, confidence penalty, and label smoothing, cutout and mixup.",
        "og_introduction": "Deep neural networks (DNNs) are developing rapidly and are widely used in many fields such as reflection removal [Amanlou et al., 2022], dust pollution [Ebrahimi-Khusfi et al., 2021], building defects detection [Perez et al., 2019], cities and urban development [Nosratabadi et al., 2020]. As more and more models are proposed in the literature, deep neural networks have shown remarkable improvements in performance. However, among various learning problems, over-fitting on training data is a great problem that affects the test accuracy. So a certain regularization method is often needed in the training process. In linear models, Ridge Regression [Hoerl and Kennard, 1970] and Lasso [Tibshirani, 1996] are usually used to avoid over-fitting. They are also called L2 and L1 regularization. L2 regularization has the effect of shrinkage while L1 regularization can be conductive to both shrinkage and sparsity. From the Bayesian perspective, L2 and L1 regularization can be interpreted with normal prior distribution and laplace prior distribution respectively. Apart from L2 and L1 regularization, there are many other forms of regularizers in DNNs. The most widely used one is Weight-Decay [Krogh and Hertz, 1992]. Loshchilov and Hutter [2019] also showed that L2 regularization and Weight-Decay are not identical. Dropout [Srivastava et al., 2014] is another method to avoid over-fitting by reducing co-adapting between units in neural networks. Dropout has inspired a large body of work studying its effects (Wager et al. [2013]; Helmbold and Long [2015]; Wei et al. [2020]). After dropout, various regularization schemes can be applied additionally.",
        "og_keywords": "AI, Deep Learning, Neural Networks",
        "og_proposed_method": "We propose a new regularization method for deep neural networks by penalizing the trace of Hessian. This regularizer is motivated by a recent guarantee bound of the generalization error. Hutchinson method is a classical unbiased estimator for the trace of a matrix, but it is very time-consuming on deep learning models. Hence a dropout scheme is proposed to efficiently implements the Hutchinson method. Then we discuss a connection to linear stability of a nonlinear dynamical system and flat/sharp minima. Experiments demonstrate that our method outperforms existing regularizers and data augmentation methods, such as Jacobian, confidence penalty, and label smoothing, cutout and mixup."
        },
  "sus": {"sus_title": "Deep Neural Network Regularization Using Stochastic Estimators of Hessian Trace",
          "sus_ps_obj": "By penalising the trace of the Hessian, we create a unique regularisation strategy for deep neural networks in this research. A recent guarantee constraint for the generalisation error served as the inspiration for this regularizer. The Hutchinson technique is a traditional unbiased estimator for the trace of a matrix, however deep learning models take a very long time to run. In order to execute the Hutchinson approach effectively, a dropout system is suggested. The relationship between flat/sharp minima and the linear stability of a nonlinear dynamical system is then covered. Studies show that our approach works better than popular regularizers and data augmentation techniques including Jacobian, confidence penalty, label smoothing, cutoff, and mixup.",
          "sus_introduction": "Deep neural networks (DNNs) are evolving quickly and are currently being used extensively in a variety of fields, including reflection removal (Amanlou et al., 2022), dust pollution (Ebrahimi-Khusfi et al., 2021), building defect detection (Perez et al., 2019), cities and urban development (Nosratabadi et al., 2020), and more. Deep neural networks have significantly improved in performance as more and more models are put forth in the literature. Overfitting on training data, however, is a significant issue that has an impact on test accuracy among other learning issues. Thus, in the training phase, a particular regularisation technique is frequently required. Ridge Regression [Hoerl and Kennard, 1970] and Lasso [Tibshirani, 1996] are commonly employed in linear models to prevent over-fitting. Additionally known as L2 and L1 regularisation. L2 regularisation has the following impact shrinkage, but both shrinkage and sparsity can be induced via L1 regularisation. L2 and L1 regularisation can be understood from a Bayesian perspective using the normal prior distribution and laplace prior distribution, respectively. There are other regularizers in DNNs besides L2 and L1 regularisation. Weight-Decay is the one that is most frequently employed [Krogh and Hertz, 1992]. Additionally, Loshchilov and Hutter [2019] demonstrated that Weight-Decay and L2 regularisation are not equivalent. Another strategy to prevent over-fitting is dropout [Srivastava et al., 2014], which reduces co-adaptation between neural network units. Large amounts of research have been done on the impacts of dropout (Wager et al. [2013]; Helmbold and Long [2015]; Wei et al. [2020]). After dropping out, alternative regularisation plans may also be used.",
          "sus_keywords": "AI, Deep Learning, Neural Networks, Regularization, Dropout, Weight Decay",
          "sus_proposed_method": "We offer a new regularisation strategy for deep neural networks that penalises the Hessian trace. A recent guarantee bound of the generalisation error spurred the development of this regularizer. The Hutchinson technique is a traditional unbiased estimator for the trace of a matrix, however it takes a long time on deep learning models. As a result, a dropout strategy is given in order to efficiently apply the Hutchinson approach. The connection between linear stability of a nonlinear dynamical system and flat/sharp minima is then discussed. Experiments show that our method outperforms existing regularizers and data augmentation techniques like Jacobian, confidence penalty, label smoothing, cutoff, and mixup."
          },
  "type": 0,
  "apikey":"AIzaSyDaS_yp6jSkBET9Z5ozTpjfFtb6C2pvYB8"
}